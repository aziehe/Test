# -*- coding: utf-8 -*-
"""Function_Interpolation_Test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wJFhSeTF9xTikN_ranR2xebf9HEaHo5Y

## Compare ChebyKAN's ability in function interpolation with MLP with proper MLP baseline
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
import time
#from ChebyKANLayer import ChebyKANLayer

# Define target function
def target_function(x):
    y = np.zeros_like(x)
    mask1 = x < 0.5
    y[mask1] = np.sin(20 * np.pi * x[mask1]) + x[mask1] ** 2
    mask2 = (0.5 <= x) & (x < 1.5)
    y[mask2] = 0.5 * x[mask2] * np.exp(-x[mask2]) + np.abs(np.sin(5 * np.pi * x[mask2]))
    mask3 = x >= 1.5
    y[mask3] = np.log(x[mask3] - 1) / np.log(2) - np.cos(2 * np.pi * x[mask3])

    return y

# Define MLP and ChebyKAN
class SimpleMLP(nn.Module):
    def __init__(self, K=128):
        super(SimpleMLP, self).__init__()
        self.layers = nn.Sequential(
            nn.Linear(1, K),
            nn.Tanh(),
            nn.Linear(K, 1)
        )

    def forward(self, x):
        # Just centralize the input
        return self.layers(x-1)


class ChebyKAN(nn.Module):
    def __init__(self):
        super(ChebyKAN, self).__init__()
        self.chebykan1 = ChebyKANLayer(1, 8, 8)
        self.chebykan2 = ChebyKANLayer(8, 1, 8)

    def forward(self, x):
        x = self.chebykan1(x)
        x = self.chebykan2(x)
        return x

# Generate sample data
x_train = torch.linspace(0, 2, steps=500).unsqueeze(1)
y_train = torch.tensor(target_function(x_train))

# Instantiate models
#cheby_model = ChebyKAN()
mlp_model = SimpleMLP()

# Define loss function and optimizer
criterion = nn.MSELoss()
#optimizer_cheby = torch.optim.Adam(cheby_model.parameters(), lr=0.01)
optimizer_mlp = torch.optim.Adam(mlp_model.parameters(), lr=0.03)
epochs = 200000
sched = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_mlp, epochs)

# Train the models

for epoch in range(epochs):
    #optimizer_cheby.zero_grad()
    #outputs_cheby = cheby_model(x_train)
    #loss_cheby = criterion(outputs_cheby, y_train)
    #loss_cheby.backward()
    #optimizer_cheby.step()

    optimizer_mlp.zero_grad()
    outputs_mlp = mlp_model(x_train)
    loss_mlp = criterion(outputs_mlp, y_train)
    loss_mlp.backward()
    optimizer_mlp.step()
    sched.step()

    if epoch % 1000 == 0:
        print(f'Epoch {epoch + 1}/{epochs}, MLP Loss: {loss_mlp.item():.4f}')

# Test the models
x_test = torch.linspace(0, 2, steps=400).unsqueeze(1)
#y_pred_cheby = cheby_model(x_test).detach()
y_pred_mlp = mlp_model(x_test).detach()

# Plot the results
plt.figure(figsize=(10, 5))
plt.plot(x_train.numpy(), y_train.numpy(), 'ro', label='Original Data')
#plt.plot(x_test.numpy(), y_pred_cheby.numpy(), 'b-', label='ChebyKAN')
plt.plot(x_test.numpy(), y_pred_mlp.numpy(), 'g--', label='MLP')
plt.title('Comparison of ChebyKAN and MLP Interpolations f(x)')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend()
plt.show()

